# Project 5: NLP Text Generation with GPT-2

This project demonstrates how to use a pre-trained GPT-2 model from HuggingFace to generate text based on a custom prompt.

## Features
- Loads GPT-2 small model
- Generates text from any user prompt
- Uses HuggingFace Transformers library
- Beginner-friendly AI project

## How to Run
1. Install dependencies: pip install transformers
2. Open the notebook: gpt2-text-generator.ipynb
3. Change the prompt to generate different text.
4. View the results in sample_output.txt

## Tools Used
- Python
- Jupyter Notebook
- HuggingFace Transformers
